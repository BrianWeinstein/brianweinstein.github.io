[{"content":"Defining clear KPIs is one of the most important things for a Product team, and (unfortunately) it’s incredibly easy to do poorly. A good metric not only helps evaluate your product’s performance, but also guides your team in building the right things to progress you toward your goal.\nThe framework described below is one I’ve used for a few years while working with teams that develop software products, and has been very helpful for me, my Data Science teammates, and our Product partners in measuring the success or failure of the products we build.\nShared definitions for objectives, KPIs, \u0026amp; product metrics Before diving in, let’s define a few relevant terms and how they relate to each other.\nAn objective is a description of the impact you want to have on the business. It should be qualitative, easy to understand, and somewhat obvious.\nA key performance indicator (KPI) (aka success metric) is the one, top-level metric that measures if your product is driving the outcome outlined in the Objective. This is the quantifiable top-level goal of your product, and should be a more quantitative statement of your objective. This is the metric you use to measure the performance of your product. Your success metric should be set longer-term and the definition of it shouldn’t change very much, if at all, from quarter to quarter.\nA product metric measures the success of a specific feature. These are lower level metrics than KPIs: product metrics are something you should monitor to make sure your feature is facilitating the user behaviors that ultimately contribute to your KPI. Product metrics are also helpful in reporting: to ensure users are making use of the feature you’ve built in the way that you intend.\nA health metric is used to monitor the technical health of a product. These are usually more pertinent to Engineers, but important for Product leaders to monitor as well. Performance metrics include things like page load time, API response time, etc. I won’t focus on them here, but want to call out that they’re distinct from these other types of metrics.\nYou may have different names for some of these concepts, but the ideas behind them should be more or less the same. What’s most important is aligning on a common vocabulary for you and your team to use when talking about these ideas.\n(a bad metric)\nThe “good metric” checklist Since it’s so easy to come up with the wrong metric, I like to think through a few criteria while brainstorming to ensure we settle on a metric suitable to the product. The criteria below apply to both KPIs and Product Metrics.\nSpecific \u0026amp; sensitive: Metrics should be specific to the product or feature, and need to be explicitly and quantitatively defined. The metric should also be sensitive enough to measure the impact we expect to see.\nRobust: To complement the sensitivity criteria above, we also need to make sure the metric is measuring only the effect of the product of interest, and that it isn’t reactive to things we expect to change but don’t control. Related to internal validity, we should try to avoid using a metric that can be significantly influenced by anything other than the product/feature we care about.\nMeasurable: This one is kind of obvious, but a metric must be something that we can actually measure. It’s not uncommon to ideate a bunch of “ideal” metrics that would perfectly measure the impact of your product, but end up being impossible or infeasible to really capture.\nInterpretable: Metrics should be easy to understand and agreed upon by those whose success is measured by the metric. There’s often a tradeoff between simplicity and accuracy, and I typically err on the side of simplicity. A metric that’s hard to understand provides none of the benefits listed in the section below.\nAligned: Objectives, KPIs, and Product Metrics are hierarchical: every product should have an objective, a KPI that quantitatively measures progress toward achieving the objective, and then multiple product metrics to evaluate the performance of individual features. A product’s KPI should also be aligned upward with higher level company metrics. Any movement in these metrics should also be reflected in and contribute to those above them in the hierarchy.\nThis certainly isn’t an exhaustive list, but captures some of the most important criteria. If your metric meets all five of these conditions you should be in fairly good shape. If your metric doesn’t meet one or more of these criteria, you’ll likely need to ideate other metrics to use for your product.\nBenefits of this approach It takes a fair amount of effort to pick the right metric, and this is why it all matters!\nClarity of thought: Often what seems obvious to you is not so clear to others. Defining your goals \u0026amp; objectives in terms of hard numbers forces you to make your personal intuition clear to your team and to others at the company.\nOpportunity for innovation: KPIs and product metrics represent a goal, but don’t mandate the path to get there. This frees up everyone on the team to think about new ways to move the needle rather than focusing on a prescribed solution.\nAlignment on goals: When you set your metrics, you tell your business partners what return it should expect from its investment in your team. If expectations aren’t aligned, you’re able to pivot before it becomes a problem.\nInsight for prioritization: You can use your pre-defined KPI to compare the potential impact of a handful of projects you’re considering working on.\nProof of success: It’s much easier to communicate your impact on the organization when your goals are quantifiable.\nIndication of failure: By monitoring progress toward your goals, it’s easy to correct course or sunset a product when your efforts aren’t as effective as intended.\nNotes on this framework \u0026amp; further reading This is a work in progress! I’ve added to and generalized this framework over the past few years as I’ve worked with different types of teams (based on their domains, operating style, degree of data literacy, etc.), and will keep doing so in the future.\nThere’s a lot I don’t touch on in this article, like leading \u0026amp; lagging indicators, reliability, defining targets for your metrics, balancing metrics, and more. For further reading, I like:\n Finding the metrics that matter for your product by the Product \u0026amp; Data team at Intercom How to grow product with KPIs and How to prioritize work with KPIs by Ilya Leyrikh A framework to define your product metrics by Zandre Coetzer 10 tips on how to choose the right key performance indicators by Roman Pichler  ","permalink":"https://www.brianweinstein.co/posts/20201222-defining-meaningful-metrics-for-product-teams/","summary":"A checklist for developing better product metrics \u0026amp; KPIs","title":"Defining meaningful metrics for product teams"},{"content":"A few years ago as a Data Scientist I was presenting to co-workers an analysis I’d been working on. The presentation went fine and the work was well-received, but I could tell the group was a little underwhelmed. Towards the end of the presentation, one co-worker asked, “Did you find anything that surprised you? Anything we didn’t already know?”\nI had uncovered some new information, but most of what I’d found was well-aligned with what we already thought to be true. Still, I understood their sentiment. Any Data Scientist or Researcher will tell you that the most common thing we find when analyzing a dataset is… nothing interesting. It happens constantly. Many of our findings corroborate what we and our business partners already thought to be true, even when we’ve asked the right question. This can be frustrating for Data Scientists, Researchers, and our partners, but finding “nothing interesting” is very different from finding “nothing useful,” and I’m a strong believer that finding nothing interesting after asking the right question is still worthy of celebration.\nThe Utility-Interest plane Before diving in, I want to emphasize the difference between a result being interesting and a result being useful. All analytical results (and the questions that spawned them) will fall somewhere in the Utility-Interest plane.\nA. Useful results that are also interesting are the holy grail. Findings from these analyses drum up tons of excitement with stakeholders and have the potential to create a huge impact.\nB. Useful results that aren’t too interesting are less exciting, but are equally valuable! These are the only types of uninteresting results that are still defensible (the main topic of this post!). Useful, yet uninteresting results often arise when evaluating a hypothesis that everyone had assumed to be true, or when tackling a question that’d been answered through other methods in the past.\nC. Useless results that aren’t very interesting are just a poor use of time. These come from asking the wrong question, and a question to which everyone already knew the answer. These won’t gain much traction with stakeholders, and the primary downside is just wasting your own time.\nD. Useless, but interesting results are dangerous. Very dangerous! Useless, yet interesting results arise when finding an exciting answer to the wrong question. Stakeholders can latch onto these findings and invest their own time into addressing a topic that should be lower priority.\nBy finding “nothing interesting” in the data (i.e., a result in quadrant B) and presenting it to your stakeholders, you’re able to make decisions with more confidence, ask meaningful follow-up questions, and increase stakeholders’ trust in using data in the future.\nKnowing when your intuition is right is just as important as knowing when it’s wrong Asking a question of the data means you’re unsure about something: maybe a course of action to take, the reason behind something happening, or something else. Exploring a dataset and finding no surprises just means that, in this case, your intuition wasn’t too far off.\nEven when you and your business partners have some intuition about a problem area, evaluating your hypotheses with data will let you know, without a doubt, if your hypotheses were true. Knowing when you’re right is just as important as knowing when you’re not, and by evaluating your hypotheses you’ve learned to either maintain or change course.\nAsking meaningful follow-ups Assuming you asked a worthwhile question of the data, finding “nothing interesting” will help inform what questions you should ask in the future. Any useful finding — whether interesting or not — gives you more confidence in the problem area and refines your area of focus, helping you to ask better questions going forward.\nReinforcing confidence in data Findings that contradict our intuition can be hard to accept — especially when the findings tell us that not only was our intuition wrong, but that our actions or plans were too. By finding and presenting “nothing interesting,” you help build trust between your stakeholders and the data, making it easier for them to accept information from you in the future, especially when it’s counter to some of their beliefs.\nWhat to do now Ask the right questions of your data. Of course, the points above only hold true if you’ve asked the right question in the first place. Poor questions can sometimes lead to interesting answers, but the usefulness of these answers will be limited. My favorite way to refine a research question is to brainstorm with a cross-functional group of stakeholders (plus with this approach, you get stakeholder buy-in at the same time).\nCelebrate “nothing interesting.” A finding doesn’t have to be interesting in order to be useful. Next time you find “nothing interesting,” remember to celebrate it.\n Further reading For resources on asking good questions, I really like Asking Great Questions as a Data Scientist by Kristen Kehrer, and How to solve a business problem using data by Laura Ellis. (Please let me know if you have any others!)\n","permalink":"https://www.brianweinstein.co/posts/20200219-in-defense-of-nothing-interesting/","summary":"A tribute to useful, but less interesting research findings","title":"In defense of “nothing interesting”"},{"content":"The Net Promoter Score is a widely-used survey question that companies use to measure customer satisfaction, loyalty, and growth.\nProponents of NPS are drawn to it because it’s a single number that appears — on the surface, at least — to be linked to some significant indicators of performance. NPS a bad measure of success, though. It uses a poorly phrased question, a response scale that’s entirely too big, and an absurd method of calculation.\nThere are other metrics you can use that will be more accurate, more interpretable, and much more predictive of satisfaction, loyalty, or growth.\nBackground The standard Net Promoter Score (NPS) question asks, “How likely is it that you would recommend [company X] to a friend or colleague?” Respondents are given a scale ranging from 0–10, with 0 labeled with “Not at all likely,” and 10 labeled with “Extremely likely.”\nUnder the NPS methodology, respondents who submit a 9 or 10 are considered “promoters,” 7 or 8 are considered “passives”, and 0–6 are considered “detractors.”\nThe Net Promoter Score for a group of respondents is defined as the percentage of respondents who are promoters minus the percentage of respondents who are detractors.\nNPS = (# of promoters - # of detractors) / (total # of respondents)\nOrigin NPS was originally proposed in a December 2003 Harvard Business Review (HBR) article by Fred Reichheld, a director at the Bain \u0026amp; Company management consultancy. Reichheld proposed the score as a “loyalty” metric, which he defines as the “willingness of someone… to make an investment or personal sacrifice in order to strengthen a relationship.”\nReichheld tested eight survey questions among 4,000 consumers, and tracked these consumers’ future purchases and referrals. He then measured the link between the survey responses and actual purchase and referral behaviors.\nThe NPS question — “How likely is it that you would recommend [company X] to a friend or colleague?” — was the 1st- or 2nd- most predictive question in 11 of Reichheld’s 14 case studies, showing “the strongest statistical correlation with repeat purchases or referrals.”\nReichheld chose a 0-to-10 scale, where 10 meant “extremely likely” to recommend and 0 meant “not at all likely.” He claimed that this scale was\n “simple and unambiguous,” “divide[s] customers into practical groups deserving different… organizational responses,” was “intuitive to customers when they assign grades,” and was intuitive “to employees and partners responsible for interpreting the results and taking action.”  I disagree with all of these. More on that below.\nReichheld then grouped the 11-point scale into the three clusters: promoters, passives, and detractors. In his analysis, Reichheld found a strong correlation between companies’ net-promoter figures and their revenue growth rates.\nI highly recommend reading the original article with a critical eye: It’s full of anecdotes and correlations that Reichheld frames as causal relationships. Every few paragraphs he attempts to argue that NPS is superior over some alternative measurement in terms of assessing loyalty, growth, etc.; but fails to sufficiently justify his claims.\nWhy do companies use NPS? NPS is popular. I mean very popular. Tons of companies ask customers the NPS question, and many use it to measure and assess their performance.\nProponents of NPS are drawn to it because it’s a single number that appears — on the surface, at least — to be linked to some significant KPIs. It’s also easy to measure and produces a statistic that changes easily over time.\nIf you’re trying to get your organization to be more data-driven, then NPS is certainly better than nothing.\nWhy you shouldn’t use NPS Now that we’ve gotten that out of the way, it’s time to discuss the many, many shortcomings of NPS. The phrasing of the NPS question, the measurement scale it uses, and method of calculation all go against the basic principles of survey sciences.\nQuestion The NPS question asks a respondent to rate the likelihood of a hypothetical future; but strong, reliable survey questions ask respondents about their past behaviors, which tend to be much more predictive than forward-looking hypotheticals. “Do you plan to begin a diet in the next 6 weeks?”, for example, is a very different question from “Did you begin a diet in the last 6 weeks?” The NPS question forces the respondent to predict an ideal, future self, as opposed to reporting on their actualized behaviors.\nThe HBR article also claims that the NPS question measures loyalty and growth. In reality, though, it fails to ask about either, and isn’t necessarily what users of NPS are attempting to measure. In many cases, survey questions should be phrased to directly measure the quantity of interest.\nAn NPS-like question asking about actualized behavior would look more like, “In the last 6 weeks, have you referred [company X] to a friend or colleague?”\nReichheld promotes the NPS question as the most accurate one in predicting revenue growth rate. Proponents of NPS often fail to realize, however, that the NPS question was the most accurate from a set of 8 poorly phrased options in Reichheld’s study. In Reichheld’s findings, the NPS question wasn’t even the most accurate predictor in all industries: In database software and computer systems, for example, other questions were stronger predictors of revenue growth rate.\nScale Responses collected from a large, 11-point scale are extremely noisy, and meaningful changes in ratings are hard to detect. On the NPS scale, the difference between a “6” and a “7” isn’t clear in the survey analysis, and the lack of labels on the intermediate (i.e., non-extreme) choices also make the distinction very subjective to respondents. The NPS scale is poorly calibrated, and so are the responses.\nA better scale would use a 3-option Yes/Maybe/No system or a similar scale with 5 options. For any survey question, the response scale and number of options should be crafted to the individual question, and an 11-option scale is likely always too big.\nMethod of calculation The method of calculation is one of the stranger aspects of the NPS.\nThe bucketing methodology that groups respondents into Promoters, Passives, and Detractors ends up hiding some improvements and exaggerating others. Even if respondents were able to make meaningful distinctions between a “5” and a “6”, or between a “4” and a “5”, the bucketing method categorizes all of these into the “Detractor” group, and these changes aren’t reflected in the score. An extreme example is when a company with all “0” ratings improves to having all “6” ratings: this is a huge improvement, but the NPS methodology makes it so the score doesn’t change at all. Some changes are also exaggerated by the method of calculation: the distinction between a “6” (detractor) and a “7” (passive), or between an “8” (passive) and a “9” (promoter) is exaggerated by the bucketing methodology.\nThe method of calculation — subtracting the percentage of detractor respondents from the percentage of promoter respondents — also produces a metric that’s difficult to interpret and hides important information. All three of the following response sets, for example, produce an NPS of +60:\nFinding an alternative measurement NPS alternatives The most commonly used alternatives to NPS entail a rephrasing of the question and usage of a smaller scale.\nIf you’re truly trying to measure growth or word-of-mouth promotion, I highly recommend Netflix’s retrospective phrasing of an NPS-style question. In its early days, Netflix asked subscribers, “In the last 6 weeks, did you recommend us to a friend or family member?” and gave respondents only a Yes/No scale to respond. Netflix also paired this with another question: they asked new subscribers, “Were you recommended to us by a friend or family member?”\nOther companies, like Vox Media’s Polygon, use a similar phrasing of the question with binary response options.\nThese are both significant improvements over the standard NPS question and scale.\nYouTube has a version that deviates less from the standard NPS question and scale, but still makes some significant improvements: They keep the standard NPS question, but instead use a smaller, labeled scale.\nSome alternatives that are even better In any survey, the quantity you’re trying to measure should dictate both the question you ask and the scale you use. The questions and scales you design to measure growth, loyalty, satisfaction, etc. should each be customized for a given use case. A few examples for different measurements are outlined below.\n Growth: In the last 3 months, have you recommended [company X] to a friend, colleague, or family member? [Yes/No] Loyalty: In the last 6 weeks, have you considered [canceling your subscription, switching to another provider, etc.]? [Yes/No] Satisfaction: How satisfied are you with [company X]? [1 (Very dissatisfied), 2 (Dissatisfied), 3 (Neither), 4 (Satisfied), 5 (Very satisfied)]  What to do now  Identify what you’re trying to measure and write an appropriate question. If you want to measure word-of-mouth promotion or estimate future growth, then the “Growth” question above would be a good start. Measuring customer loyalty or customer satisfaction require entirely different questions, so make sure to ask about the thing you’re trying to measure. Pick a reasonable scale for your question. 3- or 5-option satisfaction scales, and a Yes/No binary scale all capture accurate information and are easy for respondents to select from in a response. Use a simple, logical method of calculation. If your response scale has 5 or fewer options, then it’s easy enough to report on the entire response distribution. If you need to have a single number to use in further analysis, then you might like a top-box or top-two-box percentage. You could also use the average of the numeric-encoded values, although you lose some information this way.  Whatever you do, pick something simpler and more interpretable than NPS.\nAdditional reading and references  Net Promoter Score Considered Harmful by Jared M. Spool On Surveys by Erika Hall Measuring the WeWork Member Experience by Tomer Sharon  ","permalink":"https://www.brianweinstein.co/posts/20180124-moving-beyond-the-net-promoter-score/","summary":"A guide to building a more meaningful metric","title":"Moving beyond the Net Promoter Score"},{"content":"The first debate in the 2016 presidential race was held on September 26. It’s no secret that Clinton and Trump are running on drastically different platforms, but how do they compare when it comes to their speech patterns and word choice? To quantify this, I dug into the data, using the debate transcript and natural language processing.\nI measured the sentiment of Clinton’s and Trump’s responses, and examined how emotional their words were throughout the debate. I also looked at each candidate’s most commonly used adjectives. Building off the work of Alvin Chang at Vox, I was also able to examine how the speech patterns of Clinton and Trump each changed when directly responding to and when skirting the questions.\nSentiment Using the Google Cloud Natural Language API, I measured the sentiment of each candidate’s answers. The polarity of a response is a measure of how positive or negative it is, and the magnitude indicates how much emotion the words convey. The chart below shows the polarity of each candidate’s responses, weighted by the magnitude.\nTrump and Clinton matched each other’s polarity for the first half of the debate, but after his defense of stop-and-frisk around 9:50 PM, Trump’s words became much more negative.\nThroughout the rest of the debate — during the questions on birtherism, cyber security, homegrown terrorism, nuclear weapons, and Clinton’s looks and stamina — Clinton became more positive and Trump more negative.\nThe combination of polarity and magnitude gives us the best understanding of each line’s overall sentiment, and each candidate’s most positive and negative responses are posted here.\nBraggadocios, and other adjectives I was also interested in the adjectives each candidate used most frequently during the debate. Using syntax analysis to extract each word’s part of speech, I identified the most-used adjectives of each candidate.\nAnswers vs non-answers As Chang found, the candidates spent a lot of time not answering Holt’s questions — 48% of Clinton’s words and a whopping 69% of Trump’s words were used in non-answers — and using the data Chang compiled, I was able to look at how the candidate’s speech patterns differed when answering and not answering the questions.\nSentence subjects (“I alone can fix it”) Using part-of-speech tagging, I also identified the subjects of each candidate’s sentences. Clinton was more inclusive in her words, but only when directly responding to questions — using the plural “we” more frequently than the singular “I” — and the the opposite was true for her when avoiding a response. Trump, on the other hand, was always more likely to use “I” over “we”.\nNon-answer phrases The words each candidate used when directly answering the questions are all, unsurprisingly, highly related to the questions Holt asked. What’s interesting here are the topics the candidates defaulted to when avoiding a response.\n  A handful of my findings didn’t make it into this post. If you’re interested in more, there’s some additional analysis, including multiple classification models, in the project’s GitHub repo. The text of this article (excluding this sentence) has polarity -0.4 and magnitude 15.5, so despite my best efforts it’s leaning slightly negative. Many thanks to Alvin Chang and Vox for their permission to use their annotated transcript, and to Kelsey Scherer for designing the charts and lead image. Analysis was performed in R. Plots were generated using ggplot2, and then styled by Scherer using Sketch. The sentiment scores, part of speech tags, and all of the other NLP datasets can be found in the GitHub repo.  ","permalink":"https://www.brianweinstein.co/posts/20161003-debate-nlp/","summary":"Natural language processing on the first 2016 presidential debate","title":"Speaking like a president"},{"content":"I love frozen yogurt. When I first moved to New York three years ago, I lived only 1/8th of a mile from the closest froyo shop. The convenience of this 4-minute walk is something I neither appreciated nor utilized enough at the time.\nAfter moving to Harlem last year, it’s been harder than ever to satisfy my near-constant craving for this cold candy soup — I’m now a 24-minute walk to the nearest frozen yogurt.\nAs someone who loves data and has too much time to spare, I decided to find the locations in Manhattan with highest and lowest froyo densitiy. Inspired by Ben Wellington’s work on I Quant NY, I calculated the distance from every lot in Manhattan to the nearest froyo shop and mapped it out.\nhttps://brianweinstein.cartodb.com/viz/27dd05e0-2486-11e6-98ba-0e98b61680bf/embed_map\nThe highest density of froyo is right around West 33rd St. and 8th Ave., with three shops within a 1-block radius. The lowest density is right in Harlem. The red circle on the map shows the location farthest from frozen yogurt. The record belongs to 700 Esplanade Gardens Plaza, a co-op right by the 145th St. stop on the 3-train, with a 51-minute trek across Manhattan to the Pinkberry by Columbia.\nThe map shows all of the froyo shops in Manhattan, and you can click on any lot to find the distance to the closest shop.\n  R code posted here. All distances in the map are measured using great-circle distance (i.e., ”as the crow flies”), according to the law of cosines. Frozen yogurt locations were found via the Google Places Nearby Search API. The API returned some non-froyo-exclusive shops like Ben and Jerry’s, which I kept in the dataset since they technically serve some frozen yogurt (although we all know these shops don’t really count). I only included froyo shops that were in Manhattan, so some lots may have a closer shop than the one listed if we include those in other boroughs. Manhattan lot locations are from PLUTO. The map was created using CartoDB. Tons of inspiration for this came from Ben Wellington’s work on I Quant NY.  ","permalink":"https://www.brianweinstein.co/posts/20160531-froyo-nyc/","summary":"I love frozen yogurt. When I first moved to New York three years ago, I lived only 1/8th of a mile from the closest froyo shop. The convenience of this 4-minute walk is something I neither appreciated nor utilized enough at the time.\nAfter moving to Harlem last year, it’s been harder than ever to satisfy my near-constant craving for this cold candy soup — I’m now a 24-minute walk to the nearest frozen yogurt.","title":"Mapping the frozen yogurt shop closest to each Manhattan apartment"},{"content":"              The wave equation is a partial differential equation that describes the propagation of various types of waves.\nThe equation appears throughout many fields in physics, including acoustics, fluid dynamics, electromagnetism, and quantum mechanics. With some modifications, it can even describe the spread of traffic jams on busy highways!\nThe one-dimensional equation was first discovered by d’Alembert in 1746 as he studied how vibrations propagated through a string, and the two- and three-dimensional equations were solved soon after by Euler during his study of acoustics.\nThe simulations above show the propagation of a disturbance on a two-dimensional surface for two different sets of boundary conditions [1] [2].\n Mathematica code posted here.\n","permalink":"https://www.brianweinstein.co/posts/20150128-wave-equation/","summary":"The wave equation is a partial differential equation that describes the propagation of various types of waves.\nThe equation appears throughout many fields in physics, including acoustics, fluid dynamics, electromagnetism, and quantum mechanics. With some modifications, it can even describe the spread of traffic jams on busy highways!\nThe one-dimensional equation was first discovered by d’Alembert in 1746 as he studied how vibrations propagated through a string, and the two- and three-dimensional equations were solved soon after by Euler during his study of acoustics.","title":"Wave Equation"},{"content":"                  A Platonic solid is a polyhedron where (1) each face is the same regular polygon, and (2) each vertex joins the same number of faces.\nThe Platonic solids are highly symmetrical, and, in three dimensions, only five such solids can exist: the tetrahedron, cube, octahedron, dodecahedron, and icosahedron.\nThis was first proven in Euclid’s Elements around 300 B.C., and has since been more rigorously proven using the Euler characteristic. The proofs are relatively easy to follow, and if you’re interested you can check them out both here and here.\n Mathematica code:\npSolids={\u0026quot;Tetrahedron\u0026quot;,\u0026quot;Cube\u0026quot;,\u0026quot;Octahedron\u0026quot;,\u0026quot;Dodecahedron\u0026quot;,\u0026quot;Icosahedron\u0026quot;} Manipulate[Graphics3D[ {Opacity[0.8],Rotate[PolyhedronData[pSolids[[n]],\u0026quot;Faces\u0026quot;],th,{0,0,1}], Opacity[0],Circumsphere[PolyhedronData[pSolids[[n]], \u0026quot;VertexCoordinates\u0026quot;][[1;;4]]]}, Boxed-\u0026gt;False,SphericalRegion-\u0026gt;True],{n,1,5,1},{th,0,2\\[Pi]}] ","permalink":"https://www.brianweinstein.co/posts/20150120-platonic-solids/","summary":"A Platonic solid is a polyhedron where (1) each face is the same regular polygon, and (2) each vertex joins the same number of faces.\nThe Platonic solids are highly symmetrical, and, in three dimensions, only five such solids can exist: the tetrahedron, cube, octahedron, dodecahedron, and icosahedron.\nThis was first proven in Euclid’s Elements around 300 B.","title":"Platonic Solids"},{"content":"                      Imagine n runners on a circular track of length 1. The runners start from the same spot at the same time, and each has a distinct, constant speed. A runner is considered “lonely” whenever it is a distance of at least 1/n from every other runner. The Lonely Runner Conjecture (LRC) states that each runner will eventually, at some point in time, be lonely.\nSaid differently, the LRC states that for each runner, the spacing around it will eventually be greater than or equal to the spacing it would experience if the all of the runners were equally distributed around the track.\nThe conjecture has been proven to be true for 7 or fewer runners, but, interestingly enough, has never been proven to work for all cases of 8 or more runners. [In my 8-runner simulation above, I’ve only shown that it works for a specific set of runner speeds — I haven’t proven that it works for all sets of speeds.]\nIn the GIFs above, an arc appears around a runner whenever the runner is lonely, and the color of a runner fades after it’s been lonely at least once.\n Mathematica code posted here.\nAdditional sources not linked above: [1] [2] [3]\n","permalink":"https://www.brianweinstein.co/posts/20141225-lonely-runner-conjecture/","summary":"Imagine n runners on a circular track of length 1. The runners start from the same spot at the same time, and each has a distinct, constant speed. A runner is considered “lonely” whenever it is a distance of at least 1/n from every other runner. The Lonely Runner Conjecture (LRC) states that each runner will eventually, at some point in time, be lonely.","title":"Lonely Runner Conjecture"},{"content":"                      Two weeks ago, the ESA made history by landing a spacecraft on a comet. The spacecraft, named Philae, was carried to Comet 67P by a larger space probe named Rosetta.\nDetermining where Philae would land was a big step in this mission. Many attributes about the comet, including its topography, were taken into account. To help in this process, the ESA derived a 3D model of the comet\u0026rsquo;s surface, and the data (made up of over 30,000 measurements) was recently released.\nThe images above use ESA data to model the surface of 67P. By adjusting lighting and orientation, actual photos taken by Rosetta (images 3 and 5) can be reproduced.\nYou can play around with the model yourself!\nEither use the code below in Mathematica or on the Wolfram Programming Cloud (with a free account), or play with the model online by clicking here*.\n *This link will expire on Dec 23, 2014, after which you\u0026rsquo;ll have to upload the .obj file directly to the online viewer.\nMathematica code:\nobjLink = \u0026quot;http://sci.esa.int/science-e/www/object/doc.cfm?fobjectid=54726\u0026quot;; comet = Import[objLink, \u0026quot;OBJ\u0026quot;]; pts = Import[objLink, {\u0026quot;OBJ\u0026quot;, \u0026quot;VertexData\u0026quot;}]; ListSurfacePlot3D[pts, MaxPlotPoints -\u0026gt; 20, Mesh -\u0026gt; All, MeshStyle -\u0026gt; Opacity[0.4]] Show[comet, Background -\u0026gt; Black, Lighting -\u0026gt; {{\u0026quot;Directional\u0026quot;, LightGray, {-7, -10, 10}}}] Additional sources: [1] [2] [3]\nImages 3 \u0026amp; 5: ESA/Rosetta/NAVCAM, CC BY-SA IGO 3.0\n","permalink":"https://www.brianweinstein.co/posts/20141124-comet-67p/","summary":"Two weeks ago, the ESA made history by landing a spacecraft on a comet. The spacecraft, named Philae, was carried to Comet 67P by a larger space probe named Rosetta.\nDetermining where Philae would land was a big step in this mission. Many attributes about the comet, including its topography, were taken into account.","title":"Modeling Comet 67P"},{"content":"                  A harmonograph is a mechanical device consisting of two or more pendulums attached to a pen. The swinging pendulums control the motion of the pen, tracing out a geometric pattern on a sheet of paper.\nSince the system is damped by friction, the pattern spirals in on itself as time progresses.\nEach of the GIFs above simulate the output of a 4-pendulum system (modeled after a harmonograph as configured in this video). The different outputs are generated by using different pendulum length ratios in each simulation.\n Mathematica code posted here.\nAdditional source not linked above.\n","permalink":"https://www.brianweinstein.co/posts/20141110-harmonographs/","summary":"A harmonograph is a mechanical device consisting of two or more pendulums attached to a pen. The swinging pendulums control the motion of the pen, tracing out a geometric pattern on a sheet of paper.\nSince the system is damped by friction, the pattern spirals in on itself as time progresses.\nEach of the GIFs above simulate the output of a 4-pendulum system (modeled after a harmonograph as configured in this video).","title":"Harmonographs"},{"content":"                  A curve of constant width is a convex, two-dimensional shape that, when rotated inside a square, always makes contact with all four sides.\nA circle is the most obvious (but somewhat trivial) example. Some non-trivial examples are the odd-sided Reuleaux polygons — the first four of which are shown above.\nSince they don\u0026rsquo;t have fixed axes of rotation, curves of constant width (except the circle) have few practical applications. One notable use of the Reuleaux triangle, though, is in drilling holes in the shape of a slightly rounded square (watch one of the triangle\u0026rsquo;s vertices and notice the shape it traces out as it spins).\nOn a less technical note, all curves of constant width are solutions to the brainteaser, \u0026ldquo;Other than a circle, what shape can you make a manhole cover such that it can\u0026rsquo;t fall through the hole it covers?\u0026rdquo;\n Mathematica code posted here.\nAdditional source not linked above.\n","permalink":"https://www.brianweinstein.co/posts/20141020-curves-of-constant-width/","summary":"A curve of constant width is a convex, two-dimensional shape that, when rotated inside a square, always makes contact with all four sides.\nA circle is the most obvious (but somewhat trivial) example. Some non-trivial examples are the odd-sided Reuleaux polygons — the first four of which are shown above.\nSince they don\u0026rsquo;t have fixed axes of rotation, curves of constant width (except the circle) have few practical applications.","title":"Curves of Constant Width and Odd-Sided Reuleaux Polygons"},{"content":"              Evidence-based theories on the structure of atoms have been around since the early 1800s. Dalton’s billiard ball model was the first on the map, and with further discoveries and experiments — like Thompson’s discovery of the electron and Rutherford’s gold foil experiment — improved models of atomic structure were introduced.\nThe first GIF above shows Rutherford’s planetary model, which was proposed in 1911. In his model, negatively-charged electrons orbit an incredibly small, dense nucleus of positive charge. Despite being a completely incorrect model, most people still think this is what atoms really look like*. This is not an atom. It’s physically impossible for electrons to stably orbit like this, and the idea of orbiting electrons was entirely replaced by 1926.\nI can’t say what an atom actually looks like, but the most accurate model we have today is governed by the laws of quantum mechanics. The location of an electron is determined by a probability distribution, called an atomic orbital, which tells us the probability of an electron existing in any specific region around a nucleus. The second image shows the surface around a hydrogen nucleus on which an excited electron is most likely to exist.\n Mathematica code posted here.\n*Advertisements and popular science articles incorrectly represent atoms all the time. Even the US Atomic Energy Commission and the International Atomic Energy Agency use the Rutherford model in their logos!\n","permalink":"https://www.brianweinstein.co/posts/20140922-atomic-models/","summary":"Evidence-based theories on the structure of atoms have been around since the early 1800s. Dalton’s billiard ball model was the first on the map, and with further discoveries and experiments — like Thompson’s discovery of the electron and Rutherford’s gold foil experiment — improved models of atomic structure were introduced.\nThe first GIF above shows Rutherford’s planetary model, which was proposed in 1911.","title":"Atomic Models"},{"content":"              Cops and Robbers is a mathematical game in which pursuers (cops) attempt to capture evaders (robbers). The game is one of many pursuit-evasion games, each of which is governed by a different set of rules. The general goal of these problems is to determine the number of pursuers required to capture a given number of evaders.\nThe GIFs above show two versions of the game. The first is similar to the standard Cops and Robbers rendition, and the second is best described as \u0026ldquo;Zombies and Humans\u0026rdquo;.\nIn both versions, an evader moves in the direction that gets it furthest away from the pursuers (focusing more on the closer pursuers), and a pursuer moves in the direction that gets it closest to the evaders (focusing more on the closer evaders).\nIn the first simulation, members of both groups have a constant speed. In the second simulation, members of a group move more quickly the closer they are to members of the opposite group, and slower when further away.\n Mathematica code posted here.\nAdditional sources not linked above: [1] [2]\n","permalink":"https://www.brianweinstein.co/posts/20140902-cops-and-robbers/","summary":"Cops and Robbers is a mathematical game in which pursuers (cops) attempt to capture evaders (robbers). The game is one of many pursuit-evasion games, each of which is governed by a different set of rules. The general goal of these problems is to determine the number of pursuers required to capture a given number of evaders.","title":"Cops and Robbers (and Zombies and Humans)"},{"content":"              A reflector is a type of antenna that receives and focuses various types of signals. Reflectors have numerous applications, from satellite dishes and telescopes, to long-distance microphones and car headlights. One common feature of these examples is their parabolic shape, giving them the name parabolic reflectors.\nIt turns out that paraboloids are the perfect shape for focusing signals from distant sources. When pointed directly at the the incoming signal, a parabolic reflector (GIF 1) collects the signal to a single focal point, where a receiver, called a feed horn, is placed to collect the focused transmission.\nIn many applications, parabolic reflectors are too costly to produce, so spherical reflectors (GIF 2) are used instead. The disadvantage of spherical reflectors is that they have multiple focal points, and therefore produce blurry results.\n Mathematica code posted here.\nThis code is incredibly messy and I guarantee there’s a better way to calculate this. Please contact me if you have suggestions!\n","permalink":"https://www.brianweinstein.co/posts/20140811-signal-collection-parabolic-reflectors/","summary":"A reflector is a type of antenna that receives and focuses various types of signals. Reflectors have numerous applications, from satellite dishes and telescopes, to long-distance microphones and car headlights. One common feature of these examples is their parabolic shape, giving them the name parabolic reflectors.\nIt turns out that paraboloids are the perfect shape for focusing signals from distant sources.","title":"Signal Collection and Parabolic Reflectors"},{"content":"A Taylor series is a way to represent a function in terms of polynomials. Since polynomials are usually much easier to work with than complicated functions, Taylor series have numerous applications in both math and physics.\nThere are many equations in physics — like the one describing the motion of a pendulum — that are impossible to solve in terms of elementary functions. \u0026ldquo;Approximations using the first few terms of a Taylor series can make [these] otherwise unsolvable problems\u0026rdquo; solvable for a restricted area of interest [1].\nThe GIF above shows the five-term Taylor series approximation of a sine wave about x=0.\n Mathematica code:\nf[x_] := Sin[x] ts[x_, a_, nmax_] := Sum[(Derivative[n][f][a]/n!)*(x - a)^n, {n, 0, nmax}] Manipulate[Plot[{f[x], ts[x, 0, nmax]}, {x, -2*Pi, 2*Pi}, PlotRange -\u0026amp;gt; {-1.45, 1.45}, PlotStyle -\u0026amp;gt; {{Thick, Cyan}, {Thick, Dotted, Yellow}}, AxesStyle -\u0026amp;gt; LightGray, Background -\u0026amp;gt; Darker[Gray, 0.8]], {nmax, 1, 30, 1}] ","permalink":"https://www.brianweinstein.co/posts/20140730-taylor-series-approximations/","summary":"A Taylor series is a way to represent a function in terms of polynomials. Since polynomials are usually much easier to work with than complicated functions, Taylor series have numerous applications in both math and physics.\nThere are many equations in physics — like the one describing the motion of a pendulum — that are impossible to solve in terms of elementary functions. \u0026ldquo;Approximations using the first few terms of a Taylor series can make [these] otherwise unsolvable problems\u0026rdquo; solvable for a restricted area of interest [1].","title":"Taylor Series Approximations"},{"content":"              Time for an experiment! Find a book and secure it shut using tape or a rubber band. Now experiment with spinning the book while tossing it into the air. You’ll notice that when the book is spun about its longest or shortest axis it rotates stably, but when spun about its intermediate-length axis it quickly wobbles out of control.\nEvery rigid body has three special, or principal axes about which it can rotate. For a rectangular prism — like the book in our experiment — the principal axes run parallel to the shortest, intermediate-length, and longest edges, each going through the prism’s center of mass. These axes have the highest, intermediate, and lowest moments of inertia, respectively.\nWhen the book is tossed into the air and spun, either about its shortest or longest principal axis, it continues to rotate about that axis forever (or until it hits the floor). For these axes, this indefinite, stable rotation occurs even when the axis of rotation is slightly perturbed.\nWhen spun about its intermediate principal axis, though, the book also continues to rotate about that axis indefinitely, but only if the axis of rotation is exactly in the same direction as the intermediate principal axis. In this case, even the slightest perturbation causes the book to wobble out of control.\nThe first simulation above shows a rotation about the unstable intermediate axis, where a slight perturbation causes the book to wobble out of control. The second and third simulations show rotations about the two stable axes.\nUnfortunately, as far as my understanding goes, there\u0026rsquo;s no intuitive, non-mathematical explanation as to why rotations about the intermediate principal axis are unstable. If you\u0026rsquo;re interested, you can find the stability analysis here.\n Mathematica code posted here.\nAdditional sources not linked above: [1] [2] [3] [4]\n","permalink":"https://www.brianweinstein.co/posts/20140713-rotational-stability/","summary":"Time for an experiment! Find a book and secure it shut using tape or a rubber band. Now experiment with spinning the book while tossing it into the air. You’ll notice that when the book is spun about its longest or shortest axis it rotates stably, but when spun about its intermediate-length axis it quickly wobbles out of control.","title":"Rotational Stability"},{"content":"Gabriel’s Horn is a three-dimensional horn shape with the counterintuitive property of having a finite volume but an infinite surface area.\nThis fact results in the Painter’s Paradox — A painter could fill the horn with a finite quantity of paint, “and yet that paint would not be sufficient to coat [the horn’s] inner surface” [1].\nIf the horn’s bell had, for example, a 6-inch radius, we’d only need about a half gallon of paint to fill the horn all the way up. Even though this half gallon is enough to entirely fill the horn, it’s not enough to even coat a fraction of the inner wall!\nThe mathematical explanation is a bit confusing if you haven’t taken a first course in calculus, but if you’re interested, you can check it out here.\n Mathematica code:\nx[u_, v_] := u y[u_, v_] := Cos[v]/u z[u_, v_] := Sin[v]/u Manipulate[ParametricPlot3D[{{x[u, v], y[u, v], z[u, v]}}, {u, 1, umax}, {v, 0, 2*Pi}, PlotRange -\u0026amp;gt; {{0, 20}, {-1, 1}, {-1, 1}}, Mesh -\u0026amp;gt; {Floor[umax], 20}, Axes -\u0026amp;gt; False, Boxed -\u0026amp;gt; False], {{umax, 20}, 1.1, 20}] Additional source not linked above.\n","permalink":"https://www.brianweinstein.co/posts/20140629-painters-paradox/","summary":"Gabriel’s Horn is a three-dimensional horn shape with the counterintuitive property of having a finite volume but an infinite surface area.\nThis fact results in the Painter’s Paradox — A painter could fill the horn with a finite quantity of paint, “and yet that paint would not be sufficient to coat [the horn’s] inner surface” [1].\nIf the horn’s bell had, for example, a 6-inch radius, we’d only need about a half gallon of paint to fill the horn all the way up.","title":"Gabriel’s Horn and the Painter’s Paradox"},{"content":"The Lagrangian points are the five locations in an orbital system where the combined gravitational force of two large masses is exactly canceled out by the centrifugal force arising from the rotating reference frame.\nAt these five points, the net force on a third body (of negligible mass) is 0, allowing the third object to be completely stationary relative to the two other masses. That is, when placed at any of these points, the third body stays perfectly still in the rotating frame.\nThe first image shows the fields due to the first mass, the second mass, and the rotating reference frame. When added together, these fields generate the effective field shown in the second image. The five Lagrangian points are indicated with gray spheres.\nThe first three Lagrangian points (labeled L1, L2, and L3) lie in line with the two larger bodies and are considered metastable equilibria. L4 and L5 lie 60° ahead of and behind the second body in its orbit and are considered stable equilibria.\nLagrangian points offer unique advantages for space research, and the Lagrangian points of the Sun-Earth system are currently home to four different satellites.\n Mathematica code posted here.\nAdditional sources not linked above: [1] [2] [3] [4] [5]\n","permalink":"https://www.brianweinstein.co/posts/20140609-lagrangian-points/","summary":"The Lagrangian points are the five locations in an orbital system where the combined gravitational force of two large masses is exactly canceled out by the centrifugal force arising from the rotating reference frame.\nAt these five points, the net force on a third body (of negligible mass) is 0, allowing the third object to be completely stationary relative to the two other masses. That is, when placed at any of these points, the third body stays perfectly still in the rotating frame.","title":"Lagrangian Points"},{"content":"                  The Doppler effect is the shift in the frequency of a wave observed when the source of the wave (or the medium through which the wave travels) is moving relative to the observer.\nWe’re most familiar with the Doppler effect as it appears in sound waves traveling through air (i.e., pressure waves) – think of how the pitch of a siren drops as an emergency vehicle passes you. The first GIF shows a stationary source and the second shows a source moving to the right at 40% the speed of sound. Notice in the second GIF how the wavefronts are closer together in front of the source (producing a higher frequency) and further apart behind it (producing a lower frequency).\nThe Doppler effect is interesting in its own right, but things get much more exciting when the source travels at speeds greater than or equal to the speed of sound.\nWhen the source travels at the speed of sound (GIF 3) the source will always be at the leading edge of the waves it produces, and when traveling faster than the speed of sound (GIF 4), the source will always be in front of the waves it produces. In both of these cases, notice how the waves overlap with each other. The high pressure areas of each wave constructively interfere and produce a region of extremely high pressure (much higher than in the surrounding areas). This rapid rise in air pressure is a shock wave, and the sound associated with it is a sonic boom.\nIn each of the GIFs above we see the radiating wavefronts on the left, and the pressure distribution and interference of the waves on the right.\n Mathematica code posted here.\nAdditional source not linked above.\n","permalink":"https://www.brianweinstein.co/posts/20140528-sonic-boom-doppler-effect/","summary":"The Doppler effect is the shift in the frequency of a wave observed when the source of the wave (or the medium through which the wave travels) is moving relative to the observer.\nWe’re most familiar with the Doppler effect as it appears in sound waves traveling through air (i.e., pressure waves) – think of how the pitch of a siren drops as an emergency vehicle passes you.","title":"Sonic Booms and the Doppler Effect"},{"content":"A chaotic system is one in which infinitesimal differences in the starting conditions lead to drastically different results as the system evolves.\nSummarized by mathematician Edward Lorenz, \u0026ldquo;Chaos [is] when the present determines the future, but the approximate present does not approximately determine the future.\u0026rdquo;\nThere\u0026rsquo;s an important distinction to make between a chaotic system and a random system. Given the starting conditions, a chaotic system is entirely deterministic. A random system, on the other hand, is entirely non-deterministic, even when the starting conditions are known. That is, with enough information, the evolution of a chaotic system is entirely predictable, but in a random system there\u0026rsquo;s no amount of information that would be enough to predict the system\u0026rsquo;s evolution.\nThe simulations above show two slightly different initial conditions for a double pendulum — an example of a chaotic system. In the left animation both pendulums begin horizontally, and in the right animation the red pendulum begins horizontally and the blue is rotated by 0.1 radians (≈ 5.73°) above the positive x-axis. In both simulations, all of the pendulums begin from rest.\nFor more information on how to solve for the motion of a double pendulum, check out my video here.\n Mathematica code posted here.\n","permalink":"https://www.brianweinstein.co/posts/20140519-chaos-and-the-double-pendulum/","summary":"A chaotic system is one in which infinitesimal differences in the starting conditions lead to drastically different results as the system evolves.\nSummarized by mathematician Edward Lorenz, \u0026ldquo;Chaos [is] when the present determines the future, but the approximate present does not approximately determine the future.\u0026rdquo;\nThere\u0026rsquo;s an important distinction to make between a chaotic system and a random system. Given the starting conditions, a chaotic system is entirely deterministic. A random system, on the other hand, is entirely non-deterministic, even when the starting conditions are known.","title":"Chaos and the Double Pendulum"},{"content":"A couple of weeks ago I made a post about the classical three-body problem, which involves determining the motion of three masses interacting via gravity over time.\nIn that simulation, the three masses were restricted to a plane. Even though we live in three spatial dimensions, a two-dimensional model for celestial orbits isn’t such a bad approximation – the orbits of masses in celestial system are often within a few degrees of the same plane.\nIn the simulation above I’ve built in a third spatial dimension and modeled a system in which the masses do not stay within the same plane. In this simulation, the green and red bodies are 9 and 13 times more massive than the blue body, respectively.\n The Mathematica code for this one was a bit long, so I\u0026rsquo;ve posted it here.\n","permalink":"https://www.brianweinstein.co/posts/20140512-three-body-problem-3d/","summary":"A couple of weeks ago I made a post about the classical three-body problem, which involves determining the motion of three masses interacting via gravity over time.\nIn that simulation, the three masses were restricted to a plane. Even though we live in three spatial dimensions, a two-dimensional model for celestial orbits isn’t such a bad approximation – the orbits of masses in celestial system are often within a few degrees of the same plane.","title":"Three-Body Problem in 3D"},{"content":"              An orientable surface is a surface on which it’s possible to make a consistent definition of direction. Most surfaces we encounter – like spheres, planes, and tori (doughnut shapes) – are orientable. When visualized in three dimensions, orientable surfaces have two distinct sides.\nNon-orientable surfaces, on the other hand, have only one side. From Wikipedia, “The essence of one-sidedness is that [an] ant can crawl from one side of the surface to the ‘other’ without going through the surface or flipping over an edge, but simply by crawling far enough.” At any point on a non-orientable surface it’s impossible to uniquely define, for example, the “clockwise” direction.\nThe GIFs above show two examples of non-orientable surfaces: a Klein bottle and a Möbius strip.\n Mathematica code [Klein Bottle]:\nxk[u_, v_] := (-2/15)*Cos[u]*(3*Cos[v] - 30*Sin[u] + 90*Cos[u]^4*Sin[u] - 60*Cos[u]^6*Sin[u] + 5*Cos[u]*Cos[v]*Sin[u]) yk[u_, v_] := (-15^(-1))*Sin[u]*(3*Cos[v] - 3*Cos[u]^2*Cos[v] - 48*Cos[u]^4*Cos[v] + 48*Cos[u]^6*Cos[v] - 60*Sin[u] + 5*Cos[u]*Cos[v]*Sin[u] - 5*Cos[u]^3*Cos[v]*Sin[u] - 80*Cos[u]^5*Cos[v]*Sin[u] + 80*Cos[u]^7*Cos[v]*Sin[u]) zk[u_, v_] := (2/15)*(3 + 5*Cos[u]*Sin[u])*Sin[v] kb[u_, v_] := {xk[u, v], yk[u, v], zk[u, v]} Manipulate[ParametricPlot3D[kb[u, v], {u, 0, umax}, {v, 0, 2*Pi}, PlotRange -\u0026gt; {{-1.8, 2}, {0, 4.5}, {-0.75, 0.75}}, Axes -\u0026gt; False, Boxed -\u0026gt; False, PlotStyle -\u0026gt; {Opacity[0.65]}, Mesh -\u0026gt; {20, 11}, MeshStyle -\u0026gt; Directive[Gray, Opacity[0.65], Thickness[0.003]]], {umax, 0.001, Pi}] Mathematica code [Möbius Strip]:\nxm[u_, v_] := (1 + (v/2)*Cos[u/2])*Cos[u] ym[u_, v_] := (1 + (v/2)*Cos[u/2])*Sin[u] zm[u_, v_] := (v/2)*Sin[u/2] ms[u_, v_] := {xm[u, v], ym[u, v], zm[u, v]} Manipulate[ParametricPlot3D[ ms[u, v], {u, 0, umax}, {v, -1, 1}, PlotRange -\u0026gt; {{-1.1, 1.5}, {-1.5, 1.5}, {-0.5, 0.5}}, PlotStyle -\u0026gt; {Opacity[0.65]}, Axes -\u0026gt; False, Boxed -\u0026gt; False, Mesh -\u0026gt; {20, 5}, MeshStyle -\u0026gt; Directive[Gray, Opacity[0.65], Thickness[0.003]]], {umax, 0.001, 2*Pi}] ","permalink":"https://www.brianweinstein.co/posts/20140505-non-orientable-surfaces/","summary":"An orientable surface is a surface on which it’s possible to make a consistent definition of direction. Most surfaces we encounter – like spheres, planes, and tori (doughnut shapes) – are orientable. When visualized in three dimensions, orientable surfaces have two distinct sides.\nNon-orientable surfaces, on the other hand, have only one side. From Wikipedia, “The essence of one-sidedness is that [an] ant can crawl from one side of the surface to the ‘other’ without going through the surface or flipping over an edge, but simply by crawling far enough.","title":"Non-Orientable Surfaces"},{"content":"Given the starting positions, velocities, and masses of three objects interacting via gravity, the classical three-body problem involves determining the motions of the three particles throughout time.\nWhat’s cool about the three-body system is that it’s impossible to solve for the motions of the objects exactly. That is, we can’t write down an equation that describes the system. Instead of finding an exact solution, we solve the system numerically, which amounts to finding an accurate approximation.\nThe three-body problem is an example of a chaotic system, meaning that even a slight change in the starting conditions drastically changes the time-evolution of the system.\nThe GIF above shows a planar (i.e., two-dimensional) three-body system.\n Mathematica code:\nG = 1; time = 30; mA = 1; xA0 = 0; yA0 = 0; vxA0 = 0; vyA0 = 0; mB = 1; xB0 = 1; yB0 = 0; vxB0 = 0; vyB0 = 0; mC = 1; xC0 = 0; yC0 = 0.8; vxC0 = 0; vyC0 = 0; soln1 = NDSolve[ {mA*Derivative[2][xA][t] == -((G*mA*mB*(xA[t] - xB[t]))/((xA[t] - xB[t])^2 + (yA[t] - yB[t])^2)^(3/2)) - (G*mA*mC*(xA[t] - xC[t]))/((xA[t] - xC[t])^2 + (yA[t] - yC[t])^2)^(3/2), mA*Derivative[2][yA][t] == -((G*mA*mB*(yA[t] - yB[t]))/((xA[t] - xB[t])^2 + (yA[t] - yB[t])^2)^(3/2)) - (G*mA*mC*(yA[t] - yC[t]))/((xA[t] - xC[t])^2 + (yA[t] - yC[t])^2)^(3/2), mB*Derivative[2][xB][t] == -((G*mB*mC*(xB[t] - xC[t]))/((xB[t] - xC[t])^2 + (yB[t] - yC[t])^2)^(3/2)) - (G*mB*mA*(xB[t] - xA[t]))/((xB[t] - xA[t])^2 + (yB[t] - yA[t])^2)^(3/2), mB*Derivative[2][yB][t] == -((G*mB*mC*(yB[t] - yC[t]))/((xB[t] - xC[t])^2 + (yB[t] - yC[t])^2)^(3/2)) - (G*mB*mA*(yB[t] - yA[t]))/((xB[t] - xA[t])^2 + (yB[t] - yA[t])^2)^(3/2), mC*Derivative[2][xC][t] == -((G*mC*mA*(xC[t] - xA[t]))/((xC[t] - xA[t])^2 + (yC[t] - yA[t])^2)^(3/2)) - (G*mC*mB*(xC[t] - xB[t]))/((xC[t] - xB[t])^2 + (yC[t] - yB[t])^2)^(3/2), mC*Derivative[2][yC][t] == -((G*mC*mA*(yC[t] - yA[t]))/((xC[t] - xA[t])^2 + (yC[t] - yA[t])^2)^(3/2)) - (G*mC*mB*(yC[t] - yB[t]))/((xC[t] - xB[t])^2 + (yC[t] - yB[t])^2)^(3/2), xA[0] == xA0, yA[0] == yA0, Derivative[1][xA][0] == vxA0, Derivative[1][yA][0] == vyA0, xB[0] == xB0, yB[0] == yB0, Derivative[1][xB][0] == vxB0, Derivative[1][yB][0] == vyB0, xC[0] == xC0, yC[0] == yC0, Derivative[1][xC][0] == vxC0, Derivative[1][yC][0] == vyC0 }, {xA, yA, xB, yB, xC, yC},{t, 0, time}, MaxSteps -\u0026gt; 100000] x1[t_] := Evaluate[xA[t] /. soln1[[1,1]]] y1[t_] := Evaluate[yA[t] /. soln1[[1,2]]] x2[t_] := Evaluate[xB[t] /. soln1[[1,3]]] y2[t_] := Evaluate[yB[t] /. soln1[[1,4]]] x3[t_] := Evaluate[xC[t] /. soln1[[1,5]]] y3[t_] := Evaluate[yC[t] /. soln1[[1,6]]] Manipulate[Show[ {ParametricPlot[ {{x1[t], y1[t]}, {x2[t], y2[t]}, {x3[t], y3[t]}}, {t, tmax - 0.5, tmax}, Axes -\u0026gt; False, PlotRange -\u0026gt; {{-0.55, 1.45}, {-0.55, 1.08}}, PlotStyle -\u0026gt; {Red, Green, Blue}, GridLines -\u0026gt; {Table[0.25*x + 0.07, {x, -100, 100}], Table[0.25*y + 0.01, {y, -100, 100}]}, GridLinesStyle -\u0026gt; Directive[LightGray]]}, {Graphics[{Opacity[0.7], EdgeForm[Directive[Black]], Red, Disk[{x1[tmax], y1[tmax]}, 0.03], Green, Disk[{x2[tmax], y2[tmax]}, 0.03], Blue, Disk[{x3[tmax], y3[tmax]}, 0.03]}]}, ImageSize -\u0026gt; 600], {tmax, 6.05, 16.05}] ","permalink":"https://www.brianweinstein.co/posts/20140425-three-body-problem/","summary":"Given the starting positions, velocities, and masses of three objects interacting via gravity, the classical three-body problem involves determining the motions of the three particles throughout time.\nWhat’s cool about the three-body system is that it’s impossible to solve for the motions of the objects exactly. That is, we can’t write down an equation that describes the system. Instead of finding an exact solution, we solve the system numerically, which amounts to finding an accurate approximation.","title":"Three-Body Problem on a Plane"},{"content":"A Fourier series is a way to expand a periodic function in terms of sines and cosines. The Fourier series is named after Joseph Fourier, who introduced the series as he solved for a mathematical way to describe how heat transfers in a metal plate.\nThe GIFs above show the 8-term Fourier series approximations of the square wave and the sawtooth wave.\n Mathematica code:\nf[t_] := SawtoothWave[t] T = 1; nmax = 18; a0 = (2/T)*Integrate[f[t], {t, -(T/2), T/2}] anlist = Table[(2/T)*Integrate[f[t]*Cos[(2*Pi*n*t)/T], {t, -(T/2), T/2}], {n, 1, nmax}] bnlist = Table[(2/T)*Integrate[f[t]*Sin[(2*Pi*n*t)/T], {t, -(T/2), T/2}], {n, 1, nmax}] fs[t_, nmax_] := a0/2 + Sum[anlist[[n]]*Cos[(2*Pi*n*t)/T] + bnlist[[n]]*Sin[(2*Pi*n*t)/T], {n, 1, nmax}] Manipulate[Column[{Plot[{f[t], fs[t, nmax0]}, {t, -1, 1}, PlotRange -\u0026gt; All, AxesLabel -\u0026gt; {\u0026quot;t\u0026quot;, \u0026quot;f(t)\u0026quot;}, PlotStyle -\u0026gt; {{Thick, Black}, {Thick, Red}}, ImageSize -\u0026gt; 700, AspectRatio -\u0026gt; 1/2.8], Row[{\u0026quot;f(t)=\u0026quot;, fs[t, nmax0]}]}], {nmax0, 1, nmax, 1}] ","permalink":"https://www.brianweinstein.co/posts/20140423-fourier-series/","summary":"A Fourier series is a way to expand a periodic function in terms of sines and cosines. The Fourier series is named after Joseph Fourier, who introduced the series as he solved for a mathematical way to describe how heat transfers in a metal plate.\nThe GIFs above show the 8-term Fourier series approximations of the square wave and the sawtooth wave.\n Mathematica code:\nf[t_] := SawtoothWave[t] T = 1; nmax = 18; a0 = (2/T)*Integrate[f[t], {t, -(T/2), T/2}] anlist = Table[(2/T)*Integrate[f[t]*Cos[(2*Pi*n*t)/T], {t, -(T/2), T/2}], {n, 1, nmax}] bnlist = Table[(2/T)*Integrate[f[t]*Sin[(2*Pi*n*t)/T], {t, -(T/2), T/2}], {n, 1, nmax}] fs[t_, nmax_] := a0/2 + Sum[anlist[[n]]*Cos[(2*Pi*n*t)/T] + bnlist[[n]]*Sin[(2*Pi*n*t)/T], {n, 1, nmax}] Manipulate[Column[{Plot[{f[t], fs[t, nmax0]}, {t, -1, 1}, PlotRange -\u0026gt; All, AxesLabel -\u0026gt; {\u0026quot;t\u0026quot;, \u0026quot;f(t)\u0026quot;}, PlotStyle -\u0026gt; {{Thick, Black}, {Thick, Red}}, ImageSize -\u0026gt; 700, AspectRatio -\u0026gt; 1/2.","title":"Fourier Series"},{"content":"","permalink":"https://www.brianweinstein.co/archives/","summary":"Archives","title":"Archives"},{"content":"","permalink":"https://www.brianweinstein.co/posts/","summary":"Posts","title":"Posts"},{"content":"","permalink":"https://www.brianweinstein.co/search/","summary":"search","title":"Search"}]